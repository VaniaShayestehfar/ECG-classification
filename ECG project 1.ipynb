{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# این کد دو تا دیتا ست که هنوز کامل ران نشده ولی حجم داده هارو کم کردم \n",
    "# Adjusted code to be more computer-friendly with reduced data size, epochs, batch size, and model size\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Reduce Data Size\n",
    "# ---------------------------\n",
    "train_df_sampled = train_df.sample(n=2000, random_state=42)\n",
    "X_train_mitbih_sampled = train_df_sampled.iloc[:, :186].values\n",
    "y_train_mitbih_sampled = to_categorical(train_df_sampled[187])\n",
    "\n",
    "X_test_mitbih_sampled = X_test_mitbih[:2000]\n",
    "y_test_mitbih_sampled = y_test_mitbih[:2000]\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Reduce Batch Size\n",
    "# ---------------------------\n",
    "train_mitbih_generator = DataGenerator(X_train_mitbih_sampled, y_train_mitbih_sampled, batch_size=16)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Simplify Model\n",
    "# ---------------------------\n",
    "cnn_mitbih_model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', padding='same', input_shape=(1000, 12), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Conv1D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(y_train_mitbih_sampled.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_mitbih_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Train with fewer epochs\n",
    "# ---------------------------\n",
    "history_mitbih = cnn_mitbih_model.fit(\n",
    "    train_mitbih_generator,\n",
    "    validation_data=(X_test_mitbih_sampled, y_test_mitbih_sampled),\n",
    "    epochs=5,  # Reduced epochs for quick testing\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Plot Training Results\n",
    "# ---------------------------\n",
    "# Plot MITBIH training accuracy and loss\n",
    "plt.plot(history_mitbih.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history_mitbih.history['val_accuracy'], label='val accuracy')\n",
    "plt.title('MITBIH Accuracy over epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_mitbih.history['loss'], label='train loss')\n",
    "plt.plot(history_mitbih.history['val_loss'], label='val loss')\n",
    "plt.title('MITBIH Loss over epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  با تگ  pac va pvc این کد دیتا ست دوم با ۸۱ درصد درستی \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Convolution1D, MaxPool1D, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "import itertools\n",
    "\n",
    "# Load the data from the paths provided\n",
    "train_df = pd.read_csv(\"D:/downloads/archive (1)/mitbih_train.csv\", header=None)\n",
    "test_df = pd.read_csv(\"D:/downloads/archive (1)/mitbih_test.csv\", header=None)\n",
    "\n",
    "# Data balancing using resampling\n",
    "train_df[187] = train_df[187].astype(int)\n",
    "\n",
    "# Map the labels to the desired classes (PAC = 1, PVC = 2, Normal = 0)\n",
    "train_df[187] = train_df[187].apply(lambda x: 1 if x == 1 else (2 if x == 2 else 0))  # 1 -> PAC, 2 -> PVC, 0 -> Normal\n",
    "test_df[187] = test_df[187].apply(lambda x: 1 if x == 1 else (2 if x == 2 else 0))  # Ensure the same mapping for test data\n",
    "\n",
    "# Resample the data to handle class imbalance\n",
    "df_0 = train_df[train_df[187] == 0].sample(n=20000, random_state=42)\n",
    "df_1 = train_df[train_df[187] == 1]\n",
    "df_2 = train_df[train_df[187] == 2]\n",
    "\n",
    "# Upsample classes\n",
    "df_1_upsample = resample(df_1, replace=True, n_samples=20000, random_state=123)\n",
    "df_2_upsample = resample(df_2, replace=True, n_samples=20000, random_state=124)\n",
    "\n",
    "train_df = pd.concat([df_0, df_1_upsample, df_2_upsample])\n",
    "\n",
    "# Visualizing class distribution\n",
    "equilibre = train_df[187].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "my_circle = plt.Circle((0, 0), 0.7, color='white')\n",
    "plt.pie(equilibre, labels=['Normal', 'PAC', 'PVC'], colors=['red', 'green', 'blue'], autopct='%1.1f%%')\n",
    "p = plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()\n",
    "\n",
    "# Extracting one sample per class for visualization\n",
    "c = train_df.groupby(187, group_keys=False).apply(lambda train_df: train_df.sample(1))\n",
    "\n",
    "# Plotting an example from each class\n",
    "for i in range(3):  # 3 classes: Normal, PAC, PVC\n",
    "    plt.plot(c.iloc[i, :186])\n",
    "    plt.title(f\"Class {i}\")\n",
    "    plt.show()\n",
    "\n",
    "# Preprocessing the signals and converting to categorical\n",
    "X_train = train_df.iloc[:, :186].values\n",
    "X_test = test_df.iloc[:, :186].values\n",
    "y_train = to_categorical(train_df[187], num_classes=3)  # 3 classes: Normal, PAC, PVC\n",
    "y_test = to_categorical(test_df[187], num_classes=3)    # Same for test data\n",
    "\n",
    "# Adding Gaussian noise to the signals for data augmentation\n",
    "def add_gaussian_noise(signal):\n",
    "    noise = np.random.normal(0, 0.5, 186)\n",
    "    return (signal + noise)\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i, :] = add_gaussian_noise(X_train[i, :])\n",
    "\n",
    "# Reshaping data for CNN\n",
    "X_train = X_train.reshape(len(X_train), X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(len(X_test), X_test.shape[1], 1)\n",
    "\n",
    "# CNN Model with Dropout, L2 Regularization, and Learning Rate Reduction\n",
    "def create_model(X_train, y_train):\n",
    "    im_shape = (X_train.shape[1], 1)\n",
    "    inputs_cnn = Input(shape=(im_shape), name='inputs_cnn')\n",
    "\n",
    "    # First convolutional block\n",
    "    conv1_1 = Convolution1D(64, 6, activation='relu', kernel_regularizer=l2(0.01))(inputs_cnn)\n",
    "    conv1_1 = BatchNormalization()(conv1_1)\n",
    "    pool1 = MaxPool1D(pool_size=3, strides=2, padding=\"same\")(conv1_1)\n",
    "    dropout1 = Dropout(0.5)(pool1)\n",
    "\n",
    "    # Second convolutional block\n",
    "    conv2_1 = Convolution1D(64, 3, activation='relu', kernel_regularizer=l2(0.01))(dropout1)\n",
    "    conv2_1 = BatchNormalization()(conv2_1)\n",
    "    pool2 = MaxPool1D(pool_size=2, strides=2, padding=\"same\")(conv2_1)\n",
    "    dropout2 = Dropout(0.5)(pool2)\n",
    "\n",
    "    # Third convolutional block\n",
    "    conv3_1 = Convolution1D(64, 3, activation='relu', kernel_regularizer=l2(0.01))(dropout2)\n",
    "    conv3_1 = BatchNormalization()(conv3_1)\n",
    "    pool3 = MaxPool1D(pool_size=2, strides=2, padding=\"same\")(conv3_1)\n",
    "\n",
    "    # Fully connected layers\n",
    "    flatten = Flatten()(pool3)\n",
    "    dense_end1 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(flatten)\n",
    "    dense_end2 = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(dense_end1)\n",
    "    dropout3 = Dropout(0.5)(dense_end2)\n",
    "\n",
    "    # Output layer\n",
    "    main_output = Dense(3, activation='softmax', name='main_output')(dropout3)\n",
    "\n",
    "    model = Model(inputs=inputs_cnn, outputs=main_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model(X_train, y_train)\n",
    "\n",
    "# Callbacks with EarlyStopping and ReduceLROnPlateau\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=8),\n",
    "    ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "]\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=40, batch_size=32, validation_data=(X_test, y_test), callbacks=callbacks)\n",
    "model.load_weights('best_model.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(history, X_test, y_test, model):\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Accuracy: {scores[1] * 100:.2f}%\")\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Model Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "    plot_confusion_matrix(cm, classes=['Normal', 'PAC', 'PVC'], normalize=True, title=\"Confusion Matrix\")\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluating the model\n",
    "evaluate_model(history, X_test, y_test, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#این کد داده ست دوم با تگ های خودش که درستی ۹۷ درصد ولی موقع تشخیص خطا یکم اورفیت میشه \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "from tensorflow.keras.layers import Dense, Convolution1D, MaxPool1D, Flatten, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the data from the paths provided\n",
    "train_df = pd.read_csv(\"D:/downloads/archive (1)/mitbih_train.csv\", header=None)\n",
    "test_df = pd.read_csv(\"D:/downloads/archive (1)/mitbih_test.csv\", header=None)\n",
    "\n",
    "train_df[187] = train_df[187].astype(int)\n",
    "equilibre = train_df[187].value_counts()\n",
    "print(equilibre)\n",
    "plt.figure(figsize=(20,10))\n",
    "my_circle = plt.Circle((0, 0), 0.7, color='white')\n",
    "plt.pie(equilibre, labels=['n', 'q', 'v', 's', 'f'], colors=['red', 'green', 'blue', 'skyblue', 'orange'], autopct='%1.1f%%')\n",
    "p = plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()\n",
    "\n",
    "df_1 = train_df[train_df[187] == 1]\n",
    "df_2 = train_df[train_df[187] == 2]\n",
    "df_3 = train_df[train_df[187] == 3]\n",
    "df_4 = train_df[train_df[187] == 4]\n",
    "df_0 = (train_df[train_df[187] == 0]).sample(n=20000, random_state=42)\n",
    "\n",
    "df_1_upsample = resample(df_1, replace=True, n_samples=20000, random_state=123)\n",
    "df_2_upsample = resample(df_2, replace=True, n_samples=20000, random_state=124)\n",
    "df_3_upsample = resample(df_3, replace=True, n_samples=20000, random_state=125)\n",
    "df_4_upsample = resample(df_4, replace=True, n_samples=20000, random_state=126)\n",
    "\n",
    "train_df = pd.concat([df_0, df_1_upsample, df_2_upsample, df_3_upsample, df_4_upsample])\n",
    "equilibre = train_df[187].value_counts()\n",
    "print(equilibre)\n",
    "plt.figure(figsize=(20,10))\n",
    "my_circle = plt.Circle((0, 0), 0.7, color='white')\n",
    "plt.pie(equilibre, labels=['n', 'q', 'v', 's', 'f'], colors=['red', 'green', 'blue', 'skyblue', 'orange'], autopct='%1.1f%%')\n",
    "p = plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()\n",
    "\n",
    "# Preprocessing\n",
    "target_train = train_df[187]\n",
    "target_test = test_df[187]\n",
    "y_train = to_categorical(target_train)\n",
    "y_test = to_categorical(target_test)\n",
    "\n",
    "X_train = train_df.iloc[:, :186].values\n",
    "X_test = test_df.iloc[:, :186].values\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[1]))\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[1]))\n",
    "\n",
    "# Reshape for CNN\n",
    "X_train = X_train.reshape(len(X_train), X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(len(X_test), X_test.shape[1], 1)\n",
    "\n",
    "def network(X_train, y_train, X_test, y_test):\n",
    "    im_shape = (X_train.shape[1], 1)\n",
    "    inputs_cnn = Input(shape=(im_shape), name='inputs_cnn')\n",
    "    conv1_1 = Convolution1D(64, (6), activation='relu', input_shape=im_shape)(inputs_cnn)\n",
    "    conv1_1 = BatchNormalization()(conv1_1)\n",
    "    pool1 = MaxPool1D(pool_size=(3), strides=(2), padding=\"same\")(conv1_1)\n",
    "    \n",
    "    conv2_1 = Convolution1D(64, (3), activation='relu')(pool1)\n",
    "    conv2_1 = BatchNormalization()(conv2_1)\n",
    "    pool2 = MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv2_1)\n",
    "    \n",
    "    conv3_1 = Convolution1D(64, (3), activation='relu')(pool2)\n",
    "    conv3_1 = BatchNormalization()(conv3_1)\n",
    "    pool3 = MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv3_1)\n",
    "\n",
    "    flatten = Flatten()(pool3)\n",
    "    dense_end1 = Dense(64, activation='relu')(flatten)\n",
    "    dense_end2 = Dense(32, activation='relu')(dense_end1)\n",
    "    \n",
    "    # Add Dropout layers\n",
    "    dropout1 = Dropout(0.3)(dense_end2)  # Add dropout to prevent overfitting\n",
    "    \n",
    "    main_output = Dense(5, activation='softmax', name='main_output')(dropout1)\n",
    "\n",
    "    model = Model(inputs=inputs_cnn, outputs=main_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
    "                 ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "    # Fit model\n",
    "    history = model.fit(X_train, y_train, epochs=40, callbacks=callbacks, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    model.load_weights('best_model.h5')\n",
    "    return model, history, callbacks\n",
    "\n",
    "# Ensure target_train contains the original class labels (not one-hot encoded)\n",
    "target_train = train_df[187].values  # Convert to numpy array\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(target_train), y=target_train)\n",
    "\n",
    "# Initialize model and callbacks\n",
    "model, history, callbacks = network(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Use class weights during training\n",
    "history = model.fit(X_train, y_train, epochs=40, callbacks=callbacks, batch_size=32, validation_data=(X_test, y_test), class_weight=dict(enumerate(class_weights)))\n",
    "\n",
    "def evaluate_model(history, X_test, y_test, model):\n",
    "    scores = model.evaluate((X_test), y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1] * 100))\n",
    "\n",
    "    print(history)\n",
    "    fig1, ax_acc = plt.subplots()\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model - Accuracy')\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    fig2, ax_loss = plt.subplots()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Model- Loss')\n",
    "    plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.show()\n",
    "\n",
    "    target_names = ['0', '1', '2', '3', '4']\n",
    "\n",
    "    y_true = []\n",
    "    for element in y_test:\n",
    "        y_true.append(np.argmax(element))\n",
    "    prediction_proba = model.predict(X_test)\n",
    "    prediction = np.argmax(prediction_proba, axis=1)\n",
    "    cnf_matrix = confusion_matrix(y_true, prediction)\n",
    "\n",
    "model, history, callbacks = network(X_train, y_train, X_test, y_test)\n",
    "\n",
    "evaluate_model(history, X_test, y_test, model)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F', 'Q'], normalize=True,\n",
    "                      title='Confusion matrix, with normalization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#این کد دیتا ست دوم بدون اورفیت و ۶۸ درصد درستی با تگ های خودش\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Convolution1D, MaxPool1D, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.regularizers import l2  # Importing l2 regularizer\n",
    "import itertools\n",
    "\n",
    "# Load the data from the paths provided\n",
    "train_df = pd.read_csv(\"D:/downloads/archive (1)/mitbih_train.csv\", header=None)\n",
    "test_df = pd.read_csv(\"D:/downloads/archive (1)/mitbih_test.csv\", header=None)\n",
    "\n",
    "# Data balancing using resampling\n",
    "train_df[187] = train_df[187].astype(int)\n",
    "df_1 = train_df[train_df[187] == 1]\n",
    "df_2 = train_df[train_df[187] == 2]\n",
    "df_3 = train_df[train_df[187] == 3]\n",
    "df_4 = train_df[train_df[187] == 4]\n",
    "df_0 = (train_df[train_df[187] == 0]).sample(n=20000, random_state=42)\n",
    "\n",
    "df_1_upsample = resample(df_1, replace=True, n_samples=20000, random_state=123)\n",
    "df_2_upsample = resample(df_2, replace=True, n_samples=20000, random_state=124)\n",
    "df_3_upsample = resample(df_3, replace=True, n_samples=20000, random_state=125)\n",
    "df_4_upsample = resample(df_4, replace=True, n_samples=20000, random_state=126)\n",
    "\n",
    "# Concatenating resampled data\n",
    "train_df = pd.concat([df_0, df_1_upsample, df_2_upsample, df_3_upsample, df_4_upsample])\n",
    "\n",
    "# Visualizing class distribution\n",
    "equilibre = train_df[187].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "my_circle = plt.Circle((0, 0), 0.7, color='white')\n",
    "plt.pie(equilibre, labels=['N', 'S', 'V', 'F', 'Q'], colors=['red', 'green', 'blue', 'skyblue', 'orange'], autopct='%1.1f%%')\n",
    "p = plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()\n",
    "\n",
    "# Extracting one sample per class for visualization\n",
    "c = train_df.groupby(187, group_keys=False).apply(lambda train_df: train_df.sample(1))\n",
    "\n",
    "# Plotting an example from each class\n",
    "for i in range(5):\n",
    "    plt.plot(c.iloc[i, :186])\n",
    "    plt.title(f\"Class {i}\")\n",
    "    plt.show()\n",
    "\n",
    "# Preprocessing the signals and converting to categorical\n",
    "X_train = train_df.iloc[:, :186].values\n",
    "X_test = test_df.iloc[:, :186].values\n",
    "y_train = to_categorical(train_df[187])\n",
    "y_test = to_categorical(test_df[187])\n",
    "\n",
    "# Adding Gaussian noise to the signals for data augmentation\n",
    "def add_gaussian_noise(signal):\n",
    "    noise = np.random.normal(0, 0.5, 186)\n",
    "    return (signal + noise)\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i, :] = add_gaussian_noise(X_train[i, :])\n",
    "\n",
    "# Reshaping data for CNN\n",
    "X_train = X_train.reshape(len(X_train), X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(len(X_test), X_test.shape[1], 1)\n",
    "\n",
    "# CNN Model with Dropout and L2 Regularization\n",
    "def create_model(X_train, y_train):\n",
    "    im_shape = (X_train.shape[1], 1)\n",
    "    inputs_cnn = Input(shape=(im_shape), name='inputs_cnn')\n",
    "    \n",
    "    conv1_1 = Convolution1D(64, 6, activation='relu', kernel_regularizer=l2(0.01))(inputs_cnn)\n",
    "    conv1_1 = BatchNormalization()(conv1_1)\n",
    "    pool1 = MaxPool1D(pool_size=3, strides=2, padding=\"same\")(conv1_1)\n",
    "    dropout1 = Dropout(0.5)(pool1)\n",
    "    \n",
    "    conv2_1 = Convolution1D(64, 3, activation='relu', kernel_regularizer=l2(0.01))(dropout1)\n",
    "    conv2_1 = BatchNormalization()(conv2_1)\n",
    "    pool2 = MaxPool1D(pool_size=2, strides=2, padding=\"same\")(conv2_1)\n",
    "    dropout2 = Dropout(0.5)(pool2)\n",
    "    \n",
    "    conv3_1 = Convolution1D(64, 3, activation='relu', kernel_regularizer=l2(0.01))(dropout2)\n",
    "    conv3_1 = BatchNormalization()(conv3_1)\n",
    "    pool3 = MaxPool1D(pool_size=2, strides=2, padding=\"same\")(conv3_1)\n",
    "    \n",
    "    flatten = Flatten()(pool3)\n",
    "    dense_end1 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(flatten)\n",
    "    dense_end2 = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(dense_end1)\n",
    "    dropout3 = Dropout(0.5)(dense_end2)\n",
    "    \n",
    "    main_output = Dense(5, activation='softmax', name='main_output')(dropout3)\n",
    "\n",
    "    model = Model(inputs=inputs_cnn, outputs=main_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model(X_train, y_train)\n",
    "\n",
    "# Callbacks with ReduceLROnPlateau\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=8),\n",
    "    ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "]\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=40, batch_size=32, validation_data=(X_test, y_test), callbacks=callbacks)\n",
    "model.load_weights('best_model.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(history, X_test, y_test, model):\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Accuracy: {scores[1] * 100:.2f}%\")\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Model Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "    plot_confusion_matrix(cm, classes=['N', 'S', 'V', 'F', 'Q'], normalize=True, title=\"Confusion Matrix\")\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluating the model\n",
    "evaluate_model(history, X_test, y_test, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resample\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\distribute\\combinations.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_worker_test_base\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m def_function\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\distribute\\multi_worker_test_base.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewriter_config_pb2\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_coordinator \u001b[38;5;28;01mas\u001b[39;00m dc\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster_resolver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster_resolver \u001b[38;5;28;01mas\u001b[39;00m cluster_resolver_lib\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\distribute\\distribute_coordinator.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordinator\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitored_session\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m server_lib\n\u001b[0;32m     33\u001b[0m _thread_local \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\training\\monitored_session.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variables\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m basic_session_run_hooks\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordinator\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\summary\\summary.py:52\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tb_summary\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# exports FileWriter, FileWriterCache\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileWriter\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileWriterCache\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# pylint: enable=unused-import\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plugin_asset\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_file_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EventFileWriter\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_file_writer_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EventFileWriterV2\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\summary\\writer\\event_file_writer.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_events_writer\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfile\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#این کد دو تا دیتا ست بدون کم کردن دیتا ها\n",
    "import ast\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import wfdb\n",
    "import ast\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and Preprocess MITBIH Data\n",
    "# ---------------------------\n",
    "train_df = pd.read_csv(\"D:/downloads/archive (1)/mitbih_train.csv\", header=None)\n",
    "test_df = pd.read_csv(\"D:/downloads/archive (1)/mitbih_test.csv\", header=None)\n",
    "\n",
    "train_df[187] = train_df[187].astype(int)\n",
    "\n",
    "# Upsample classes to balance dataset\n",
    "df_1 = train_df[train_df[187] == 1]\n",
    "df_2 = train_df[train_df[187] == 2]\n",
    "df_3 = train_df[train_df[187] == 3]\n",
    "df_4 = train_df[train_df[187] == 4]\n",
    "df_0 = (train_df[train_df[187] == 0]).sample(n=20000, random_state=42)\n",
    "\n",
    "df_1_upsample = resample(df_1, replace=True, n_samples=20000, random_state=123)\n",
    "df_2_upsample = resample(df_2, replace=True, n_samples=20000, random_state=124)\n",
    "df_3_upsample = resample(df_3, replace=True, n_samples=20000, random_state=125)\n",
    "df_4_upsample = resample(df_4, replace=True, n_samples=20000, random_state=126)\n",
    "\n",
    "train_df = pd.concat([df_0, df_1_upsample, df_2_upsample, df_3_upsample, df_4_upsample])\n",
    "\n",
    "# Preprocessing for MITBIH Data\n",
    "X_train_mitbih = train_df.iloc[:, :186].values\n",
    "X_test_mitbih = test_df.iloc[:, :186].values\n",
    "y_train_mitbih = to_categorical(train_df[187])\n",
    "y_test_mitbih = to_categorical(test_df[187])\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_mitbih = scaler.fit_transform(X_train_mitbih)\n",
    "X_test_mitbih = scaler.transform(X_test_mitbih)\n",
    "\n",
    "# Reshaping for CNN (make it 1000 time steps)\n",
    "X_train_mitbih = np.pad(X_train_mitbih, ((0, 0), (0, 1000 - X_train_mitbih.shape[1])), 'constant')\n",
    "X_test_mitbih = np.pad(X_test_mitbih, ((0, 0), (0, 1000 - X_test_mitbih.shape[1])), 'constant')\n",
    "\n",
    "# Make the MITBIH data have 12 channels by repeating the same data across 12 channels\n",
    "X_train_mitbih = np.repeat(X_train_mitbih[:, :, np.newaxis], 12, axis=2)\n",
    "X_test_mitbih = np.repeat(X_test_mitbih[:, :, np.newaxis], 12, axis=2)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load and Preprocess PTB-XL Data\n",
    "# ---------------------------\n",
    "path_to_csv = r\"E:\\ptbxl_database.csv\"  # Updated path\n",
    "base_path = r\"D:\\downloads\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\"\n",
    "\n",
    "df = pd.read_csv(path_to_csv)\n",
    "df['scp_codes'] = df['scp_codes'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Map SCP codes to superclass labels\n",
    "scp_to_superclass = {\n",
    "    'NORM': 'NORM', 'SR': 'NORM',\n",
    "    'IMI': 'MI', 'AMI': 'MI', 'ASMI': 'MI',\n",
    "    'NST_': 'STTC', 'NDT': 'STTC',\n",
    "    'RBBB': 'CD', 'LBBB': 'CD', 'IVCD': 'CD',\n",
    "    'LVH': 'HYP', 'HYP': 'HYP'\n",
    "}\n",
    "\n",
    "def derive_superclass(row):\n",
    "    for code in row['scp_codes'].keys():\n",
    "        if code in scp_to_superclass:\n",
    "            return scp_to_superclass[code]\n",
    "    return None\n",
    "\n",
    "df['superclass'] = df.apply(derive_superclass, axis=1)\n",
    "filtered_df = df[df['superclass'].notnull()].copy()\n",
    "\n",
    "superclass_map = {'NORM': 0, 'MI': 1, 'STTC': 2, 'CD': 3, 'HYP': 4}\n",
    "filtered_df['label'] = filtered_df['superclass'].map(superclass_map)\n",
    "\n",
    "# Check if filtered_df is empty\n",
    "print(f\"Filtered dataframe shape: {filtered_df.shape}\")\n",
    "if filtered_df.shape[0] == 0:\n",
    "    print(\"No records in filtered_df.\")\n",
    "else:\n",
    "    print(\"Filtered dataframe has records.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Load Signals Function with Debugging\n",
    "# ---------------------------\n",
    "def load_signals(df, max_length=1000):\n",
    "    data, labels = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        record_path = os.path.join(base_path, row['filename_lr'])\n",
    "        \n",
    "        if not (os.path.isfile(record_path + \".hea\") and os.path.isfile(record_path + \".dat\")):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            signal, _ = wfdb.rdsamp(record_path)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {row['filename_lr']}...\")  # Debugging line to check which records are being loaded\n",
    "        signal = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            signal.T, maxlen=max_length, padding='post', truncating='post', dtype='float32'\n",
    "        ).T\n",
    "        \n",
    "        data.append(signal)\n",
    "        labels.append(row['label'])\n",
    "\n",
    "    print(f\"Loaded {len(data)} signals and labels.\")  # Check how many valid signals and labels were loaded\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "X_ptb, y_ptb = load_signals(filtered_df, max_length=1000)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. One-hot encode PTB labels\n",
    "# ---------------------------\n",
    "y_ptb_encoded = to_categorical(y_ptb)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Use Data Generators to Avoid Memory Overload\n",
    "# ---------------------------\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X_data, y_data, batch_size=32, shuffle=True):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(X_data))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.X_data) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_X = self.X_data[batch_indices]\n",
    "        batch_y = self.y_data[batch_indices]\n",
    "        return batch_X, batch_y\n",
    "\n",
    "train_mitbih_generator = DataGenerator(X_train_mitbih, y_train_mitbih, batch_size=32)\n",
    "train_ptb_generator = DataGenerator(X_ptb, y_ptb_encoded, batch_size=32)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. CNN Model for MITBIH with Increased Regularization\n",
    "# ---------------------------\n",
    "cnn_mitbih_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', padding='same', input_shape=(1000, 12), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.5),  # Increased dropout\n",
    "    \n",
    "    Conv1D(128, kernel_size=5, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.5),  # Increased dropout\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    Dropout(0.5),  # Increased dropout\n",
    "    \n",
    "    Dense(y_train_mitbih.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_mitbih_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Train the MITBIH model\n",
    "# ---------------------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
    "\n",
    "history_mitbih = cnn_mitbih_model.fit(\n",
    "    train_mitbih_generator,\n",
    "    validation_data=(X_test_mitbih, y_test_mitbih),  # Add validation data\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Save the MITBIH model\n",
    "# ---------------------------\n",
    "cnn_mitbih_model.save('mitbih_cnn_model.keras', save_format='keras')\n",
    "\n",
    "# ---------------------------\n",
    "# 9. CNN Model for PTB-XL with Increased Regularization\n",
    "# ---------------------------\n",
    "cnn_ptb_model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', padding='same', input_shape=(1000, 12), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.5),  # Increased dropout\n",
    "    \n",
    "    Conv1D(128, kernel_size=5, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.5),  # Increased dropout\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    Dropout(0.5),  # Increased dropout\n",
    "    \n",
    "    Dense(y_ptb_encoded.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_ptb_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ---------------------------\n",
    "# 10. Train the PTB-XL model\n",
    "# ---------------------------\n",
    "history_ptb = cnn_ptb_model.fit(\n",
    "    train_ptb_generator,\n",
    "    validation_data=(X_test, y_test_ptb_encoded),  # Add validation data\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 11. Save the PTB-XL model\n",
    "# ---------------------------\n",
    "cnn_ptb_model.save('ptbxl_cnn_model.keras', save_format='keras')\n",
    "\n",
    "# ---------------------------\n",
    "# 12. Plot Training and Validation Accuracy and Loss\n",
    "# ---------------------------\n",
    "\n",
    "# MITBIH accuracy and loss\n",
    "plt.plot(history_mitbih.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history_mitbih.history['val_accuracy'], label='val accuracy')\n",
    "plt.title('MITBIH Accuracy over epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_mitbih.history['loss'], label='train loss')\n",
    "plt.plot(history_mitbih.history['val_loss'], label='val loss')\n",
    "plt.title('MITBIH Loss over epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# PTB-XL accuracy and loss\n",
    "plt.plot(history_ptb.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history_ptb.history['val_accuracy'], label='val accuracy')\n",
    "plt.title('PTB-XL Accuracy over epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_ptb.history['loss'], label='train loss')\n",
    "plt.plot(history_ptb.history['val_loss'], label='val loss')\n",
    "plt.title('PTB-XL Loss over epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#این کد دیتا ست اول با ۹۵ درصد درسنی \n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import wfdb  \n",
    "import os  \n",
    "from tensorflow.keras.models import Sequential  \n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense  \n",
    "from tensorflow.keras.utils import to_categorical  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "# Set the path to your dataset  \n",
    "path = r'C:\\Users\\Asus\\Downloads\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3'  \n",
    "\n",
    "# Load the main data CSV file  \n",
    "metadata = pd.read_csv(os.path.join(path, 'ptbxl_database.csv'))  \n",
    "\n",
    "# Define a function to load the signal data  \n",
    "def load_signals(df):  \n",
    "    data = []  \n",
    "    labels = []  \n",
    "    \n",
    "    for index, row in df.iterrows():  \n",
    "        # Get the base filename and adjust as needed  \n",
    "        base_filename = row['filename_lr']  \n",
    "        \n",
    "        # Folder path remains static since all files appear in '00000'  \n",
    "        record_folder = os.path.join(path)  \n",
    "        \n",
    "        # Construct the complete path for .hea and .dat files  \n",
    "        record_path = os.path.join(record_folder, base_filename)  # Points to the .hea and .dat files  \n",
    "        \n",
    "        print(f\"Loading: {record_path}\")  # Debug statement  \n",
    "        \n",
    "        try:  \n",
    "            # Check for required files and load them  \n",
    "            if os.path.isfile(f\"{record_path}.hea\") and os.path.isfile(f\"{record_path}.dat\"):  \n",
    "                signal, _ = wfdb.rdsamp(record_path)  # Read the signal  \n",
    "                \n",
    "                # Append to data and labels based on scp_codes  \n",
    "                if 'PAC' in row['scp_codes']:  \n",
    "                    data.append(signal)  \n",
    "                    labels.append(1)  # Label for PAC  \n",
    "                elif 'NORM' in row['scp_codes']:  \n",
    "                    data.append(signal)  \n",
    "                    labels.append(0)  # Label for Normal  \n",
    "            else:  \n",
    "                print(f\"Required files not found for {record_path}\")  \n",
    "        except Exception as e:  \n",
    "            print(f\"Error loading {record_path}: {e}\")  \n",
    "        \n",
    "    return np.array(data), np.array(labels)  \n",
    "\n",
    "# Load the data  \n",
    "X, y = load_signals(metadata)  \n",
    "\n",
    "# Check if any data was loaded  \n",
    "if X.size == 0:  \n",
    "    raise ValueError(\"No signal data was loaded; please check your file paths and structure.\")  \n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)  \n",
    "y = to_categorical(y)  \n",
    "\n",
    "# Split the dataset into training and testing sets (80-20 split)  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "\n",
    "# Build the CNN Model  \n",
    "model = Sequential()  \n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X.shape[1], X.shape[2])))  # Adjust input_shape to match number of channels  \n",
    "model.add(MaxPooling1D(pool_size=2))  \n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))  \n",
    "model.add(MaxPooling1D(pool_size=2))  \n",
    "model.add(Flatten())  \n",
    "model.add(Dense(64, activation='relu'))  \n",
    "model.add(Dense(y.shape[1], activation='softmax'))  # Number of classes  \n",
    "\n",
    "# Compile the model  \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "\n",
    "# Train the model  \n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)  \n",
    "\n",
    "# Evaluate the model  \n",
    "loss, accuracy = model.evaluate(X_test, y_test)  \n",
    "print(f'Test Loss: {loss}')  \n",
    "print(f'Test Accuracy: {accuracy}')  \n",
    "\n",
    "# Save the model if needed  \n",
    "model.save('pac_detection_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#این کد دیتا ست اول با راستی ازماییی دیتا ها با درستی ۷۷ درصد \n",
    "import os\n",
    "import ast\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ---------------------------\n",
    "# 1. بارگذاری دیتاست\n",
    "# ---------------------------\n",
    "path_to_csv = r\"C:\\Users\\Asus\\Desktop\\ptbxl_database.csv\"\n",
    "\n",
    "base_path = r\"D:\\downloads\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\"\n",
    "\n",
    "df = pd.read_csv(path_to_csv)\n",
    "df['scp_codes'] = df['scp_codes'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# ---------------------------\n",
    "# 2. نگاشت کلاس‌ها\n",
    "# ---------------------------\n",
    "scp_to_superclass = {\n",
    "    'NORM': 'NORM', 'SR': 'NORM',\n",
    "    'IMI': 'MI', 'AMI': 'MI', 'ASMI': 'MI',\n",
    "    'NST_': 'STTC', 'NDT': 'STTC',\n",
    "    'RBBB': 'CD', 'LBBB': 'CD', 'IVCD': 'CD',\n",
    "    'LVH': 'HYP', 'HYP': 'HYP'\n",
    "}\n",
    "\n",
    "def derive_superclass(row):\n",
    "    for code in row['scp_codes'].keys():\n",
    "        if code in scp_to_superclass:\n",
    "            return scp_to_superclass[code]\n",
    "    return None\n",
    "\n",
    "df['superclass'] = df.apply(derive_superclass, axis=1)\n",
    "filtered_df = df[df['superclass'].notnull()].copy()\n",
    "\n",
    "superclass_map = {'NORM': 0, 'MI': 1, 'STTC': 2, 'CD': 3, 'HYP': 4}\n",
    "filtered_df['label'] = filtered_df['superclass'].map(superclass_map)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. بارگذاری سیگنال‌های ECG\n",
    "# ---------------------------\n",
    "def load_signals(df, max_length=1000):\n",
    "    data, labels, ids = [], [], []\n",
    "    for _, row in df.iterrows():\n",
    "        record_path = os.path.join(base_path, row['filename_lr'])\n",
    "\n",
    "        if not (os.path.isfile(record_path + \".hea\") and os.path.isfile(record_path + \".dat\")):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            signal, _ = wfdb.rdsamp(record_path)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        signal = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            signal.T, maxlen=max_length, padding='post', truncating='post', dtype='float32'\n",
    "        ).T\n",
    "        \n",
    "        data.append(signal)\n",
    "        labels.append(row['label'])\n",
    "        ids.append(row['filename_lr'])\n",
    "\n",
    "    return np.array(data), np.array(labels), ids\n",
    "\n",
    "X, y, ids = load_signals(filtered_df, max_length=1000)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. تقسیم داده‌ها به مجموعه آموزش و آزمون\n",
    "# ---------------------------\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(\n",
    "        X, y, ids, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "except ValueError:\n",
    "    X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(\n",
    "        X, y, ids, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# 5. پردازش اولیه: نرمال‌سازی داده‌ها\n",
    "# ---------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = X_train.reshape(-1, 1000, X.shape[2])\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = X_test.reshape(-1, 1000, X.shape[2])\n",
    "\n",
    "# برچسب‌ها به قالب one-hot\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat  = to_categorical(y_test)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. مدل CNN-LSTM\n",
    "# ---------------------------\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=5, activation='relu', padding='same', input_shape=(1000, X.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv1D(128, kernel_size=5, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv1D(256, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(50),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(y_train_cat.shape[1], activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 7. آموزش مدل\n",
    "# ---------------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. ارزیابی مدل\n",
    "# ---------------------------\n",
    "loss, acc = model.evaluate(X_test, y_test_cat, verbose=1)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {acc:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 9. پیش‌بینی و نمایش نمونه‌ها\n",
    "# ---------------------------\n",
    "preds = model.predict(X_test)\n",
    "rev_map = {v: k for k, v in superclass_map.items()}\n",
    "\n",
    "rand_idx = np.random.choice(len(X_test), 5, replace=False)\n",
    "for idx in rand_idx:\n",
    "    true_label = rev_map[y_test[idx]]\n",
    "    pred_label = rev_map[np.argmax(preds[idx])]\n",
    "    print(f\"ID: {ids_test[idx]}, Predicted: {pred_label}, True: {true_label}\")\n",
    "\n",
    "# ذخیره مدل\n",
    "model.save('my_ecg_model.keras', save_format='keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#این کد دیتا ست اول با راستی ازماییی دیتا ها با درستی ۷۸ درصد\n",
    "import os\n",
    "import ast\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, LSTM, GlobalAveragePooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# ---------------------------\n",
    "# 1. بارگذاری دیتاست\n",
    "# ---------------------------\n",
    "path_to_csv = r\"E:\\ptbxl_database.csv\"\n",
    "base_path   = r\"D:\\downloads\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\"\n",
    "\n",
    "df = pd.read_csv(path_to_csv)\n",
    "df['scp_codes'] = df['scp_codes'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# ---------------------------\n",
    "# 2. نگاشت کلاس‌ها\n",
    "# ---------------------------\n",
    "scp_to_superclass = {\n",
    "    'NORM': 'NORM', 'SR': 'NORM',\n",
    "    'IMI': 'MI', 'AMI': 'MI', 'ASMI': 'MI',\n",
    "    'NST_': 'STTC', 'NDT': 'STTC',\n",
    "    'RBBB': 'CD', 'LBBB': 'CD', 'IVCD': 'CD',\n",
    "    'LVH': 'HYP', 'HYP': 'HYP'\n",
    "}\n",
    "\n",
    "def derive_superclass(row):\n",
    "    for code in row['scp_codes'].keys():\n",
    "        if code in scp_to_superclass:\n",
    "            return scp_to_superclass[code]\n",
    "    return None\n",
    "\n",
    "df['superclass'] = df.apply(derive_superclass, axis=1)\n",
    "filtered_df = df[df['superclass'].notnull()].copy()\n",
    "\n",
    "superclass_map = {'NORM': 0, 'MI': 1, 'STTC': 2, 'CD': 3, 'HYP': 4}\n",
    "filtered_df['label'] = filtered_df['superclass'].map(superclass_map)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. بارگذاری سیگنال‌های ECG\n",
    "# ---------------------------\n",
    "def load_signals(df, max_length=1000):\n",
    "    data, labels, ids = [], [], []\n",
    "    for _, row in df.iterrows():\n",
    "        record_path = os.path.join(base_path, row['filename_lr'])\n",
    "\n",
    "        if not (os.path.isfile(record_path + \".hea\") and os.path.isfile(record_path + \".dat\")):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            signal, _ = wfdb.rdsamp(record_path)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        signal = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            signal.T, maxlen=max_length, padding='post', truncating='post', dtype='float32'\n",
    "        ).T\n",
    "        \n",
    "        data.append(signal)\n",
    "        labels.append(row['label'])\n",
    "        ids.append(row['filename_lr'])\n",
    "\n",
    "    return np.array(data), np.array(labels), ids\n",
    "\n",
    "X, y, ids = load_signals(filtered_df, max_length=1000)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. تقسیم داده‌ها به مجموعه آموزش و آزمون\n",
    "# ---------------------------\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(\n",
    "        X, y, ids, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "except ValueError:\n",
    "    X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(\n",
    "        X, y, ids, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# 5. پردازش اولیه: نرمال‌سازی داده‌ها\n",
    "# ---------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = X_train.reshape(-1, 1000, X.shape[2])\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = X_test.reshape(-1, 1000, X.shape[2])\n",
    "\n",
    "# برچسب‌ها به قالب one-hot\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat  = to_categorical(y_test)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. مدل CNN-LSTM\n",
    "# ---------------------------\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', padding='same', input_shape=(1000, X.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv1D(256, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    GlobalAveragePooling1D(),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(y_train_cat.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Learning rate scheduler to reduce learning rate when validation loss plateaus\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 7. آموزش مدل\n",
    "# ---------------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. ارزیابی مدل\n",
    "# ---------------------------\n",
    "loss, acc = model.evaluate(X_test, y_test_cat, verbose=1)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {acc:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 9. پیش‌بینی و نمایش نمونه‌ها\n",
    "# ---------------------------\n",
    "preds = model.predict(X_test)\n",
    "rev_map = {v: k for k, v in superclass_map.items()}\n",
    "\n",
    "rand_idx = np.random.choice(len(X_test), 5, replace=False)\n",
    "for idx in rand_idx:\n",
    "    true_label = rev_map[y_test[idx]]\n",
    "    pred_label = rev_map[np.argmax(preds[idx])]\n",
    "    print(f\"ID: {ids_test[idx]}, Predicted: {pred_label}, True: {true_label}\")\n",
    "\n",
    "# ذخیره مدل\n",
    "model.save('my_ecg_model_improved.keras', save_format='keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#این کد دو تا دیتا ست ولی جدا از هم \n",
    "import os\n",
    "import ast\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "import numpy as np  # linear algebra\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load Dataset\n",
    "# ---------------------------\n",
    "path_to_csv = r\"E:\\ptbxl_database.csv\"\n",
    "base_path = r\"D:\\downloads\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\"\n",
    "\n",
    "df = pd.read_csv(path_to_csv)\n",
    "df['scp_codes'] = df['scp_codes'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Map Superclass Labels\n",
    "# ---------------------------\n",
    "scp_to_superclass = {\n",
    "    'NORM': 'NORM', 'SR': 'NORM',\n",
    "    'IMI': 'MI', 'AMI': 'MI', 'ASMI': 'MI',\n",
    "    'NST_': 'STTC', 'NDT': 'STTC',\n",
    "    'RBBB': 'CD', 'LBBB': 'CD', 'IVCD': 'CD',\n",
    "    'LVH': 'HYP', 'HYP': 'HYP'\n",
    "}\n",
    "\n",
    "def derive_superclass(row):\n",
    "    for code in row['scp_codes'].keys():\n",
    "        if code in scp_to_superclass:\n",
    "            return scp_to_superclass[code]\n",
    "    return None\n",
    "\n",
    "df['superclass'] = df.apply(derive_superclass, axis=1)\n",
    "filtered_df = df[df['superclass'].notnull()].copy()\n",
    "\n",
    "superclass_map = {'NORM': 0, 'MI': 1, 'STTC': 2, 'CD': 3, 'HYP': 4}\n",
    "filtered_df['label'] = filtered_df['superclass'].map(superclass_map)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Load ECG Signals\n",
    "# ---------------------------\n",
    "def load_signals(df, max_length=1000):\n",
    "    data, labels, ids = [], [], []\n",
    "    for _, row in df.iterrows():\n",
    "        record_path = os.path.join(base_path, row['filename_lr'])\n",
    "\n",
    "        if not (os.path.isfile(record_path + \".hea\") and os.path.isfile(record_path + \".dat\")):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            signal, _ = wfdb.rdsamp(record_path)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        signal = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            signal.T, maxlen=max_length, padding='post', truncating='post', dtype='float32'\n",
    "        ).T\n",
    "        \n",
    "        data.append(signal)\n",
    "        labels.append(row['label'])\n",
    "        ids.append(row['filename_lr'])\n",
    "\n",
    "    return np.array(data), np.array(labels), ids\n",
    "\n",
    "X, y, ids = load_signals(filtered_df, max_length=1000)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Train-Test Split\n",
    "# ---------------------------\n",
    "X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(\n",
    "    X, y, ids, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Data Preprocessing: Normalization\n",
    "# ---------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = X_train.reshape(-1, 1000, X.shape[2])\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = X_test.reshape(-1, 1000, X.shape[2])\n",
    "\n",
    "# One-hot encoding of labels\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat  = to_categorical(y_test)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. CNN-LSTM Model\n",
    "# ---------------------------\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', padding='same', input_shape=(1000, X.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv1D(256, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    GlobalAveragePooling1D(),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(y_train_cat.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Learning rate scheduler to reduce learning rate when validation loss plateaus\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Train the Model\n",
    "# ---------------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Model Evaluation\n",
    "# ---------------------------\n",
    "loss, acc = model.evaluate(X_test, y_test_cat, verbose=1)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {acc:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 9. Prediction and Display Samples\n",
    "# ---------------------------\n",
    "preds = model.predict(X_test)\n",
    "rev_map = {v: k for k, v in superclass_map.items()}\n",
    "\n",
    "rand_idx = np.random.choice(len(X_test), 5, replace=False)\n",
    "for idx in rand_idx:\n",
    "    true_label = rev_map[y_test[idx]]\n",
    "    pred_label = rev_map[np.argmax(preds[idx])]\n",
    "    print(f\"ID: {ids_test[idx]}, Predicted: {pred_label}, True: {true_label}\")\n",
    "\n",
    "# Save the model\n",
    "model.save('my_ecg_model_improved.keras', save_format='keras')\n",
    "\n",
    "# ---------------------------\n",
    "# Visualize Dataset\n",
    "# ---------------------------\n",
    "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
    "    nunique = df.nunique()\n",
    "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]]  # For displaying purposes, pick columns with between 1 and 50 unique values\n",
    "    nRow, nCol = df.shape\n",
    "    columnNames = list(df)\n",
    "    \n",
    "    # Ensure nGraphRow is an integer by using np.ceil to round up\n",
    "    nGraphRow = int(np.ceil(nCol / nGraphPerRow))\n",
    "    \n",
    "    plt.figure(num=None, figsize=(6 * nGraphPerRow, 8 * nGraphRow), dpi=80, facecolor='w', edgecolor='k')\n",
    "    \n",
    "    for i in range(min(nCol, nGraphShown)):\n",
    "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
    "        columnDf = df.iloc[:, i]\n",
    "        if not np.issubdtype(type(columnDf.iloc[0]), np.number):\n",
    "            valueCounts = columnDf.value_counts()\n",
    "            valueCounts.plot.bar()\n",
    "        else:\n",
    "            columnDf.hist()\n",
    "        plt.ylabel('counts')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title(f'{columnNames[i]} (column {i})')\n",
    "    \n",
    "    plt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)\n",
    "    plt.show()\n",
    "\n",
    "# Loop over files in the directory to load and plot the data\n",
    "for dirname, _, filenames in os.walk(\"D:/downloads/archive (1)/\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Load and visualize data for mitbih_test.csv\n",
    "nRowsRead = 1000  # specify 'None' if want to read whole file\n",
    "df1 = pd.read_csv(\"D:/downloads/archive (1)/mitbih_test.csv\", delimiter=',', nrows=nRowsRead)\n",
    "df1.dataframeName = 'mitbih_test.csv'\n",
    "nRow, nCol = df1.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')\n",
    "df1.head(5)\n",
    "plotPerColumnDistribution(df1, 10, 5)\n",
    "plotCorrelationMatrix(df1, 25)\n",
    "plotScatterMatrix(df1, 20, 10)\n",
    "\n",
    "# Load and visualize data for mitbih_train.csv\n",
    "nRowsRead = 1000  # specify 'None' if want to read whole file\n",
    "df2 = pd.read_csv(\"D:/downloads/archive (1)/mitbih_train.csv\", delimiter=',', nrows=nRowsRead)\n",
    "df2.dataframeName = 'mitbih_train.csv'\n",
    "nRow, nCol = df2.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')\n",
    "df2.head(5)\n",
    "plotPerColumnDistribution(df2, 10, 5)\n",
    "plotCorrelationMatrix(df2, 25)\n",
    "plotScatterMatrix(df2, 20, 10)\n",
    "\n",
    "# Load and visualize data for ptbdb_abnormal.csv\n",
    "nRowsRead = 1000  # specify 'None' if want to read whole file\n",
    "df3 = pd.read_csv(\"D:/downloads/archive (1)/ptbdb_abnormal.csv\", delimiter=',', nrows=nRowsRead)\n",
    "df3.dataframeName = 'ptbdb_abnormal.csv'\n",
    "nRow, nCol = df3.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')\n",
    "df3.head(5)\n",
    "plotPerColumnDistribution(df3, 10, 5)\n",
    "plotCorrelationMatrix(df3, 25)\n",
    "plotScatterMatrix(df3, 20, 10)\n",
    "\n",
    "nRowsRead = 1000  # specify 'None' if want to read whole file\n",
    "df4 = pd.read_csv(\"D:/downloads/archive (1)/ptbdb_normal.csv\", delimiter=',', nrows=nRowsRead)\n",
    "df4.dataframeName = 'ptbdb_normal.csv'\n",
    "nRow, nCol = df4.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')\n",
    "df4.head(5)\n",
    "plotPerColumnDistribution(df4, 10, 5)\n",
    "plotCorrelationMatrix(df4, 25)\n",
    "plotScatterMatrix(df4, 20, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
